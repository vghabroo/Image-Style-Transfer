{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YzOY5K69pjF"
   },
   "source": [
    "### Dependencies and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3MpR73b5Fmcr",
    "outputId": "9b0909dc-6b89-4488-9495-840552f4791f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.0.1-cp38-cp38-win_amd64.whl (172.4 MB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.15.2-cp38-cp38-win_amd64.whl (1.2 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ghabr\\anaconda3\\lib\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ghabr\\anaconda3\\lib\\site-packages (from torch) (2.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ghabr\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\ghabr\\anaconda3\\lib\\site-packages (from torch) (1.8)\n",
      "Requirement already satisfied: filelock in c:\\users\\ghabr\\anaconda3\\lib\\site-packages (from torch) (3.0.12)\n",
      "Requirement already satisfied: requests in c:\\users\\ghabr\\anaconda3\\lib\\site-packages (from torchvision) (2.25.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\ghabr\\anaconda3\\lib\\site-packages (from torchvision) (1.20.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\ghabr\\anaconda3\\lib\\site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\ghabr\\anaconda3\\lib\\site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\ghabr\\anaconda3\\lib\\site-packages (from networkx->torch) (5.0.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\ghabr\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\ghabr\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ghabr\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ghabr\\anaconda3\\lib\\site-packages (from requests->torchvision) (2020.12.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ghabr\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Installing collected packages: torch, torchvision\n",
      "Successfully installed torch-2.0.1 torchvision-0.15.2\n",
      "Requirement already satisfied: pillow in c:\\users\\ghabr\\anaconda3\\lib\\site-packages (9.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision\n",
    "!pip3 install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "COmt1Bc59aur"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDyErsNx9nGq"
   },
   "source": [
    "### Load images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Msw3mbmP9bVy"
   },
   "outputs": [],
   "source": [
    "imsize = 400  \n",
    "\n",
    "loader = transforms.Compose([\n",
    "    transforms.Resize(imsize),\n",
    "    transforms.CenterCrop(imsize),\n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ZGUJ7ge19ba7"
   },
   "outputs": [],
   "source": [
    "\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = loader(image).unsqueeze(0)\n",
    "#    unsqueeze: (1, C, H, W), where C is the number of channels (3 for RGB images), H is the height, and W is the width.\n",
    "    return image.to(device, torch.float)\n",
    "\n",
    "\n",
    "style1_img = image_loader(\"/Users/shubhangi/Downloads/istockphoto-944812540-612x612.jpg\")\n",
    "style2_img = image_loader(\"/Users/shubhangi/Downloads/istockphoto-944812540-612x612.jpg\")\n",
    "# content_img = image_loader(\"/Users/shubhangi/Downloads/istockphoto-944812540-612x612.jpg\")\n",
    "content_img = image_loader(\"/Users/shubhangi/Downloads/WhatsApp Image 2023-05-11 at 10.52.50 PM.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "CAlChkGE97IW",
    "outputId": "5221db6a-686b-4d07-dfec-3b9d44154213"
   },
   "outputs": [],
   "source": [
    "unloader = transforms.ToPILImage()\n",
    "plt.ion() \n",
    "\n",
    "def imshow(tensor, title=None):\n",
    "    image = tensor.cpu().clone()   \n",
    "    image = image.squeeze(0)\n",
    "    image = unloader(image)\n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "plt.figure()\n",
    "imshow(style1_img, title='Style 1')\n",
    "\n",
    "plt.figure()\n",
    "imshow(style2_img, title='Style 2')\n",
    "\n",
    "plt.figure()\n",
    "imshow(content_img, title='Content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ye0js3xjDlJG"
   },
   "source": [
    "### Style loss and Content loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "FwB3kofJDqFi"
   },
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "        def __init__(self, target,):\n",
    "            super(ContentLoss, self).__init__()\n",
    "            self.target = target.detach()\n",
    "            self.loss = F.mse_loss(self.target, self.target)\n",
    "\n",
    "        def forward(self, input):\n",
    "            self.loss = F.mse_loss(input, self.target)\n",
    "            return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "sXPvlcgSDrW5"
   },
   "outputs": [],
   "source": [
    "def gram_matrix(input):\n",
    "        batch_size, h, w, f_map_num = input.size()\n",
    "        features = input.view(batch_size * h, w * f_map_num)\n",
    "        G = torch.mm(features, features.t())\n",
    "        return G.div(batch_size * h * w * f_map_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "cmpMtgmSsVvb"
   },
   "outputs": [],
   "source": [
    "class StyleLoss(nn.Module):\n",
    "        def __init__(self, target_features):\n",
    "            super(StyleLoss, self).__init__()\n",
    "            self.target_gram_matrices = []\n",
    "            self.target_features = target_features\n",
    "            self.loss = torch.zeros(2)\n",
    "\n",
    "        def forward(self, input):\n",
    "            G = []\n",
    "            \n",
    "            mask2 = torch.zeros(1, 1, input.size()[2], input.size()[2])\n",
    "            mask2[:, :, :, 0:(input.size()[2] // 2)] = 1\n",
    "            new_mask2 = mask2.repeat(1,input.size()[1],1,1).to(device)\n",
    "\n",
    "            mask1 = torch.zeros(1, 1, input.size()[2], input.size()[2])\n",
    "            mask1[:, :, :, (input.size()[2] // 2):input.size()[2]] = 1\n",
    "            new_mask1 = mask1.repeat(1,input.size()[1],1,1).to(device)\n",
    "\n",
    "            masks = [new_mask2, new_mask2]\n",
    "\n",
    "            for i, target in enumerate(self.target_features):\n",
    "               target = target.unsqueeze(0) * masks[i]\n",
    "               self.target_gram_matrices.append(gram_matrix(target))\n",
    "\n",
    "            G.append(gram_matrix(input * new_mask2))\n",
    "            G.append(gram_matrix(input * new_mask1))\n",
    "            \n",
    "            if input.shape[0] != 1:\n",
    "              pass\n",
    "            else:\n",
    "              self.loss = []\n",
    "              for g, target in zip(G, self.target_gram_matrices):\n",
    "                self.loss.append(F.mse_loss(g, target))\n",
    "            return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "b7EU1WNqDrjL"
   },
   "outputs": [],
   "source": [
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "9mFkSM-rDrqN"
   },
   "outputs": [],
   "source": [
    "class Normalization(nn.Module):\n",
    "        def __init__(self, mean, std):\n",
    "            super(Normalization, self).__init__()\n",
    "#             \n",
    "            self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "            self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "        def forward(self, img):\n",
    "            return (img - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "dL_ySHaiGI9X"
   },
   "outputs": [],
   "source": [
    "content_layers_default = ['conv_4']\n",
    "style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "WaSWa5JUGNe3"
   },
   "outputs": [],
   "source": [
    "cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "# cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "wLN6SKOSDrx8"
   },
   "outputs": [],
   "source": [
    "def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n",
    "                                   style_imgs, content_img,\n",
    "                                   content_layers=content_layers_default,\n",
    "                                   style_layers=style_layers_default):\n",
    "        cnn = copy.deepcopy(cnn)\n",
    "        normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
    "\n",
    "        # just in order to have an iterable access to or list of content/syle\n",
    "        # losses\n",
    "        content_losses = []\n",
    "        style_losses = []\n",
    "\n",
    "        # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential\n",
    "        # to put in modules that are supposed to be activated sequentially\n",
    "        model = nn.Sequential(normalization)\n",
    "\n",
    "        i = 0  # increment every time we see a conv\n",
    "        for layer in cnn.children():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                i += 1\n",
    "                name = 'conv_{}'.format(i)\n",
    "            elif isinstance(layer, nn.ReLU):\n",
    "                name = 'relu_{}'.format(i)\n",
    "                # The in-place version doesn't play very nicely with the ContentLoss\n",
    "                # and StyleLoss we insert below. So we replace with out-of-place\n",
    "                # ones here.\n",
    "                layer = nn.ReLU(inplace=False)\n",
    "            elif isinstance(layer, nn.MaxPool2d):\n",
    "                name = 'pool_{}'.format(i)\n",
    "            elif isinstance(layer, nn.BatchNorm2d):\n",
    "                name = 'bn_{}'.format(i)\n",
    "            else:\n",
    "                raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "\n",
    "            model.add_module(name, layer)\n",
    "\n",
    "            if name in content_layers:\n",
    "                target = model(content_img).detach()\n",
    "                content_loss = ContentLoss(target)\n",
    "                model.add_module(\"content_loss_{}\".format(i), content_loss)\n",
    "                content_losses.append(content_loss)\n",
    "\n",
    "            if name in style_layers:\n",
    "                target_features = model(style_imgs).detach()\n",
    "                style_loss = StyleLoss(target_features)\n",
    "                model.add_module(\"style_loss_{}\".format(i), style_loss)\n",
    "                style_losses.append(style_loss)\n",
    "\n",
    "        # now we trim off the layers after the last content and style losses\n",
    "        for i in range(len(model) - 1, -1, -1):\n",
    "            if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "                break\n",
    "\n",
    "        model = model[:(i + 1)]\n",
    "\n",
    "        return model, style_losses, content_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "kbr6aRI-KEcH"
   },
   "outputs": [],
   "source": [
    "def get_input_optimizer(input_img):\n",
    "        # this line to show that input is a parameter that requires a gradient\n",
    "        optimizer = optim.LBFGS([input_img.requires_grad_()]) \n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "XXrnqzKGKGIy"
   },
   "outputs": [],
   "source": [
    "def run_style_transfer(cnn, normalization_mean, normalization_std,\n",
    "                        content_img, style_imgs, input_img, num_steps=500,\n",
    "                        style_weight=100000, content_weight=1, style1_weight=0.5, style2_weight=0.5):\n",
    "        print('Building the style transfer model..')\n",
    "        model, style_losses, content_losses = get_style_model_and_losses(cnn,\n",
    "            normalization_mean, normalization_std, style_imgs, content_img)\n",
    "        optimizer = get_input_optimizer(input_img)\n",
    "\n",
    "        print('Optimizing..')\n",
    "        run = [0]\n",
    "        while run[0] <= num_steps:\n",
    "\n",
    "            def closure():\n",
    "                input_img.data.clamp_(0, 1)\n",
    "                optimizer.zero_grad()\n",
    "                model(input_img)\n",
    "\n",
    "                style_score = 0\n",
    "                content_score = 0\n",
    "\n",
    "                for sl in style_losses:\n",
    "                    style_score += style1_weight * sl.loss[0] + style2_weight * sl.loss[1]\n",
    "                for cl in content_losses:\n",
    "                    content_score += cl.loss\n",
    "\n",
    "                style_score *= style_weight\n",
    "                content_score *= content_weight\n",
    "\n",
    "                loss = style_score + content_score\n",
    "                loss.backward()\n",
    "\n",
    "                run[0] += 1\n",
    "                if run[0] % 50 == 0:\n",
    "                    print(\"run {}:\".format(run))\n",
    "                    print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n",
    "                        style_score.item(), content_score.item()))\n",
    "                    print()\n",
    "\n",
    "                return style_score + content_score\n",
    "\n",
    "            optimizer.step(closure)\n",
    "        input_img.data.clamp_(0, 1)\n",
    "        return input_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wl93OBRpDsFG"
   },
   "outputs": [],
   "source": [
    "input_img = content_img.clone()\n",
    "# if you want to use white noise instead uncomment the below line:\n",
    "# input_img = torch.randn(content_img.data.size(), device=device)\n",
    "\n",
    "style_imgs = torch.cat([style1_img, style2_img])\n",
    "plt.figure()\n",
    "imshow(input_img, title='Input Image')\n",
    "output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "                            content_img, style_imgs, input_img, num_steps=100, style1_weight=0.4, style2_weight=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vfl6gfdXGN2g"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "imshow(output, title='Output Image')\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "  # s1-s5 num_steps=500, style1_weight=0.5, style2_weight=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
